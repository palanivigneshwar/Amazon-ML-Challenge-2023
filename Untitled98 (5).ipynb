{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train=pd.read_csv(\"/content/drive/MyDrive/Amazon ML 2023/train.csv\")\n",
        "test=pd.read_csv(\"/content/drive/MyDrive/Amazon ML 2023/test.csv\")"
      ],
      "metadata": {
        "id": "-RKzwzTWGwQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "r0j9CNY7Hlg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "train['TITLE']=train['TITLE'].fillna('')\n",
        "train['DESCRIPTION']=train['DESCRIPTION'].fillna('')\n",
        "train['BULLET_POINTS']=train['BULLET_POINTS'].fillna('')\n",
        "test['TITLE']=test['TITLE'].fillna('')\n",
        "test['DESCRIPTION']=test['DESCRIPTION'].fillna('')\n",
        "test['BULLET_POINTS']=test['BULLET_POINTS'].fillna('')\n",
        "# split the data into training and validation sets\n",
        "train_df, val_df = train_test_split(train, test_size=0.2)\n",
        "\n",
        "# create tokenizer and encode the text data\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(train_df['TITLE'].tolist() + train_df['DESCRIPTION'].tolist() + train_df['BULLET_POINTS'].tolist())\n",
        "\n",
        "train_title_seq = tokenizer.texts_to_sequences(train_df['TITLE'].tolist())\n",
        "#train_desc_seq = tokenizer.texts_to_sequences(train_df['DESCRIPTION'].tolist())\n",
        "#train_bullet_seq = tokenizer.texts_to_sequences(train_df['BULLET_POINTS'].tolist())\n",
        "\n",
        "val_title_seq = tokenizer.texts_to_sequences(val_df['TITLE'].tolist())\n",
        "#val_desc_seq = tokenizer.texts_to_sequences(val_df['DESCRIPTION'].tolist())\n",
        "#val_bullet_seq = tokenizer.texts_to_sequences(val_df['BULLET_POINTS'].tolist())\n",
        "\n",
        "test_title_seq = tokenizer.texts_to_sequences(test['TITLE'].tolist())\n",
        "#test_desc_seq = tokenizer.texts_to_sequences(test['DESCRIPTION'].tolist())\n",
        "#test_bullet_seq = tokenizer.texts_to_sequences(test['BULLET_POINTS'].tolist())\n",
        "\n",
        "# pad the sequences to have equal length\n",
        "max_seq_length = 50\n",
        "train_title_seq = tf.keras.preprocessing.sequence.pad_sequences(train_title_seq, maxlen=max_seq_length)\n",
        "#train_desc_seq = tf.keras.preprocessing.sequence.pad_sequences(train_desc_seq, maxlen=max_seq_length)\n",
        "#train_bullet_seq = tf.keras.preprocessing.sequence.pad_sequences(train_bullet_seq, maxlen=max_seq_length)\n",
        "\n",
        "val_title_seq = tf.keras.preprocessing.sequence.pad_sequences(val_title_seq, maxlen=max_seq_length)\n",
        "#val_desc_seq = tf.keras.preprocessing.sequence.pad_sequences(val_desc_seq, maxlen=max_seq_length)\n",
        "#val_bullet_seq = tf.keras.preprocessing.sequence.pad_sequences(val_bullet_seq, maxlen=max_seq_length)\n",
        "\n",
        "test_title_seq = tf.keras.preprocessing.sequence.pad_sequences(test_title_seq, maxlen=max_seq_length)\n",
        "#test_desc_seq = tf.keras.preprocessing.sequence.pad_sequences(test_desc_seq, maxlen=max_seq_length)\n",
        "#test_bullet_seq = tf.keras.preprocessing.sequence.pad_sequences(test_bullet_seq, maxlen=max_seq_length)\n",
        "'''"
      ],
      "metadata": {
        "id": "sGI10aMwerdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iupdjJ0MGi_H",
        "outputId": "5d0aa43e-c30c-40e3-9dc2-7edd4948a459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "28122/28122 [==============================] - 210s 7ms/step - loss: 3868.7461 - val_loss: 1939.4191\n",
            "Epoch 2/5\n",
            "28122/28122 [==============================] - 148s 5ms/step - loss: 3826.0732 - val_loss: 1922.2854\n",
            "Epoch 3/5\n",
            "28122/28122 [==============================] - 155s 6ms/step - loss: 3807.6606 - val_loss: 1907.7988\n",
            "Epoch 4/5\n",
            "28122/28122 [==============================] - ETA: 0s - loss: 3794.9333"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "train['TITLE']=train['TITLE'].fillna('')\n",
        "train['DESCRIPTION']=train['DESCRIPTION'].fillna('')\n",
        "train['BULLET_POINTS']=train['BULLET_POINTS'].fillna('')\n",
        "test['TITLE']=test['TITLE'].fillna('')\n",
        "test['DESCRIPTION']=test['DESCRIPTION'].fillna('')\n",
        "test['BULLET_POINTS']=test['BULLET_POINTS'].fillna('')\n",
        "# split the data into training and validation sets\n",
        "train_df, val_df = train_test_split(train, test_size=0.2)\n",
        "\n",
        "# create tokenizer and encode the text data\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(train_df['TITLE'].tolist() + train_df['DESCRIPTION'].tolist() + train_df['BULLET_POINTS'].tolist())\n",
        "\n",
        "train_title_seq = tokenizer.texts_to_sequences(train_df['TITLE'].tolist())\n",
        "train_desc_seq = tokenizer.texts_to_sequences(train_df['DESCRIPTION'].tolist())\n",
        "train_bullet_seq = tokenizer.texts_to_sequences(train_df['BULLET_POINTS'].tolist())\n",
        "\n",
        "val_title_seq = tokenizer.texts_to_sequences(val_df['TITLE'].tolist())\n",
        "val_desc_seq = tokenizer.texts_to_sequences(val_df['DESCRIPTION'].tolist())\n",
        "val_bullet_seq = tokenizer.texts_to_sequences(val_df['BULLET_POINTS'].tolist())\n",
        "\n",
        "# pad the sequences to have equal length\n",
        "max_seq_length = 50\n",
        "train_title_seq = tf.keras.preprocessing.sequence.pad_sequences(train_title_seq, maxlen=max_seq_length)\n",
        "train_desc_seq = tf.keras.preprocessing.sequence.pad_sequences(train_desc_seq, maxlen=max_seq_length)\n",
        "train_bullet_seq = tf.keras.preprocessing.sequence.pad_sequences(train_bullet_seq, maxlen=max_seq_length)\n",
        "\n",
        "val_title_seq = tf.keras.preprocessing.sequence.pad_sequences(val_title_seq, maxlen=max_seq_length)\n",
        "val_desc_seq = tf.keras.preprocessing.sequence.pad_sequences(val_desc_seq, maxlen=max_seq_length)\n",
        "val_bullet_seq = tf.keras.preprocessing.sequence.pad_sequences(val_bullet_seq, maxlen=max_seq_length)\n",
        "\n",
        "# create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_seq_length))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='mean_absolute_error', optimizer=Adam(lr=0.001))\n",
        "\n",
        "# train the model\n",
        "history = model.fit(train_title_seq, train_df['PRODUCT_LENGTH'], \n",
        "                    validation_data=(val_title_seq, val_df['PRODUCT_LENGTH']), \n",
        "                    epochs=5, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/Amazon ML 2023/CNN1D.h5')"
      ],
      "metadata": {
        "id": "MUzItjzCXJH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.models.load_model('/content/drive/MyDrive/Amazon ML 2023/CNN1D.h5')"
      ],
      "metadata": {
        "id": "0tjZybSTYP15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred=model.predict(test_title_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZHhr_UUXdJE",
        "outputId": "54c57b15-cd63-4a28-92ab-f8d120989bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22961/22961 [==============================] - 93s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(Y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWY5suZ-Yt-r",
        "outputId": "038a5eb4-e86e-4e37-e5f7-dad5321d667c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "734736"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "-o7R0y4khdaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=[]\n",
        "for i in tqdm(range(len(Y_pred))):\n",
        "  a.append(Y_pred[i][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vClgQ0AuhKnG",
        "outputId": "c83f7205-09d5-419d-ec2f-c4f81c85e0cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 734736/734736 [00:00<00:00, 1403552.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df = pd.DataFrame({\n",
        "    \"PRODUCT_ID\": test[\"PRODUCT_ID\"],\n",
        "    \"PRODUCT_LENGTH\": a\n",
        "})\n",
        "submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "lpU8711MYo_M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}